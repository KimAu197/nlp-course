{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7186371d-a854-4609-8158-8537db05c369",
   "metadata": {},
   "source": [
    "## Instruction\n",
    "\n",
    "> 1. Rename Assignment-01-###.ipynb where ### is your student ID.\n",
    "> 2. The deadline of Assignment-01 is 23:59pm, 03-31-2024\n",
    ">\n",
    "> 3. In this assignment, you will\n",
    ">    1) explore Wikipedia text data\n",
    ">    2) build language models\n",
    ">    3) build NB and LR classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c23f2dd-e01e-4b02-9b17-c92885f8a428",
   "metadata": {},
   "source": [
    "## Task0 - Download datasets\n",
    "> Download the preprocessed data, enwiki-train.json and enwiki-test.json from the Assignment-01 folder. In the data file, each line contains a Wikipedia page with attributes, title, label, and text. There are 1000 records in the train file and 100 records in test file with ten categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cc7cb5-dd64-4886-8cd5-24f147288941",
   "metadata": {},
   "source": [
    "## Task1 - Data exploring and preprocessing\n",
    "\n",
    "> 1) Print out how many documents are in each class  (for both train and test dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7904e49a",
   "metadata": {},
   "source": [
    "代码参考demo code L01以及L02 nltk 数据包的学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18ba34ae-48ae-438a-b3dd-c9d4cb3ee416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of documents in each class for train dataset:\n",
      "Film : 100\n",
      "Book : 100\n",
      "Politician : 100\n",
      "Writer : 100\n",
      "Food : 100\n",
      "Actor : 70\n",
      "Animal : 80\n",
      "Software : 130\n",
      "Artist : 100\n",
      "Disease : 120\n",
      "Count of documents in each class for test dataset:\n",
      "Film : 10\n",
      "Book : 10\n",
      "Politician : 10\n",
      "Writer : 10\n",
      "Food : 10\n",
      "Actor : 10\n",
      "Animal : 10\n",
      "Software : 10\n",
      "Artist : 10\n",
      "Disease : 10\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "with open('enwiki-train.json', 'r') as file:\n",
    "    train_data = file.read()\n",
    "\n",
    "json_objects_train = train_data.strip().split('\\n')\n",
    "\n",
    "# 提取所有class\n",
    "classes_train = []\n",
    "for obj in json_objects_train:\n",
    "    json_data = json.loads(obj)\n",
    "    classes = json_data.get('label')\n",
    "    if classes:\n",
    "        classes_train.append(classes)\n",
    "\n",
    "train_classes_count = Counter(classes_train)\n",
    "print(\"Count of documents in each class for train dataset:\")\n",
    "for classes, count in train_classes_count.items():\n",
    "    print(classes, \":\", count)\n",
    "    \n",
    "with open('enwiki-test.json', 'r') as file: \n",
    "    test_data = file.read()\n",
    "\n",
    "json_objects_test = test_data.strip().split('\\n')\n",
    "\n",
    "# 提取所有class\n",
    "classes_test = []\n",
    "for obj in json_objects_test:\n",
    "    json_data = json.loads(obj)\n",
    "    classes = json_data.get('label')\n",
    "    if classes:\n",
    "        classes_test.append(classes)\n",
    "\n",
    "test_classes_count = Counter(classes_test)\n",
    "\n",
    "print(\"Count of documents in each class for test dataset:\")\n",
    "for classes, count in test_classes_count.items():\n",
    "    print(classes, \":\", count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1a2d5c-b719-41ec-8f14-04e092517eb9",
   "metadata": {},
   "source": [
    "> 2) Print out the average number of sentences in each class.\n",
    ">    You may need to use sentence tokenization of NLTK.\n",
    ">    (for both train and test dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37e85dc7-d50b-406a-9550-b2063f236ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In train data:\n",
      "average number of sentenses in Food is: 175.24000000000004\n",
      "average number of sentenses in Animal is: 70.375\n",
      "average number of sentenses in Film is: 438.56000000000023\n",
      "average number of sentenses in Software is: 260.9538461538461\n",
      "average number of sentenses in Writer is: 420.32000000000016\n",
      "average number of sentenses in Book is: 400.36000000000024\n",
      "average number of sentenses in Politician is: 706.1999999999999\n",
      "average number of sentenses in Artist is: 306.46999999999997\n",
      "average number of sentenses in Actor is: 76.69999999999997\n",
      "average number of sentenses in Disease is: 404.90000000000015\n",
      "In test data:\n",
      "average number of sentenses in Food is: 107.60000000000001\n",
      "average number of sentenses in Animal is: 46.800000000000004\n",
      "average number of sentenses in Film is: 364.70000000000005\n",
      "average number of sentenses in Software is: 160.1\n",
      "average number of sentenses in Writer is: 294.90000000000003\n",
      "average number of sentenses in Book is: 295.90000000000003\n",
      "average number of sentenses in Politician is: 597.6\n",
      "average number of sentenses in Artist is: 233.99999999999997\n",
      "average number of sentenses in Actor is: 30.7\n",
      "average number of sentenses in Disease is: 311.7\n"
     ]
    }
   ],
   "source": [
    "# Your code goes to here\n",
    "import nltk\n",
    "## train\n",
    "def count_sent_average_train(label, dataset):\n",
    "    average_num = 0\n",
    "    docu_num = train_classes_count.get(label)\n",
    "    for i in range(len(dataset)):\n",
    "        json_data = json.loads(dataset[i])\n",
    "        classes = json_data.get('label')\n",
    "        text = json_data.get('text')\n",
    "        if classes == label:\n",
    "            sentence = nltk.sent_tokenize(text)\n",
    "            average_num += len(sentence) / docu_num\n",
    "            \n",
    "    return average_num\n",
    "print(\"In train data:\")\n",
    "for i in set(classes_train):\n",
    "    print('average number of sentenses in', i, 'is: {}'.format(count_sent_average_train(i, json_objects_train)))\n",
    "\n",
    "## test\n",
    "def count_sent_average_test(label, dataset):\n",
    "    average_num = 0\n",
    "    docu_num = test_classes_count.get(label)\n",
    "    for i in range(len(dataset)):\n",
    "        json_data = json.loads(dataset[i])\n",
    "        classes = json_data.get('label')\n",
    "        text = json_data.get('text')\n",
    "        if classes == label:\n",
    "            sentence = nltk.sent_tokenize(text)\n",
    "            average_num += len(sentence) / docu_num\n",
    "            \n",
    "    return average_num\n",
    "print(\"In test data:\")\n",
    "for i in set(classes_test):\n",
    "    print('average number of sentenses in', i, 'is: {}'.format(count_sent_average_test(i, json_objects_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502ef409-65a5-4d07-9b69-c5986569970a",
   "metadata": {},
   "source": [
    "> 3) Print out the average number of tokens in each class\n",
    ">    (for both train and test dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f4d7628e-762c-49fe-804a-796fa0265af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In train data:\n",
      "average number of tokens in Film is: 11895.279999999999\n",
      "average number of tokens in Actor is: 1868.8428571428574\n",
      "average number of tokens in Writer is: 11849.91\n",
      "average number of tokens in Disease is: 9322.958333333332\n",
      "average number of tokens in Food is: 3904.149999999999\n",
      "average number of tokens in Animal is: 1521.9250000000002\n",
      "average number of tokens in Software is: 6302.300000000002\n",
      "average number of tokens in Politician is: 18644.3\n",
      "average number of tokens in Artist is: 8212.909999999996\n",
      "average number of tokens in Book is: 10540.509999999998\n",
      "In test data:\n",
      "average number of tokens in Film is: 9292.9\n",
      "average number of tokens in Actor is: 677.5\n",
      "average number of tokens in Writer is: 8499.4\n",
      "average number of tokens in Disease is: 6988.8\n",
      "average number of tokens in Food is: 2445.5\n",
      "average number of tokens in Animal is: 885.6\n",
      "average number of tokens in Software is: 3972.7999999999993\n",
      "average number of tokens in Politician is: 15204.300000000003\n",
      "average number of tokens in Artist is: 5706.400000000001\n",
      "average number of tokens in Book is: 7711.100000000001\n"
     ]
    }
   ],
   "source": [
    "# Your code goes to here\n",
    "## train\n",
    "def count_token_average_train(label, dataset):\n",
    "    average_num = 0\n",
    "    docu_num = train_classes_count.get(label)\n",
    "    for i in range(len(dataset)):\n",
    "        json_data = json.loads(dataset[i])\n",
    "        classes = json_data.get('label')\n",
    "        text = json_data.get(\"text\")\n",
    "        if classes == label:\n",
    "            token = nltk.word_tokenize(text)\n",
    "            average_num += len(token) / docu_num\n",
    "    return average_num\n",
    "\n",
    "print(\"In train data:\")\n",
    "for i in set(classes_train):\n",
    "    print('average number of tokens in', i, 'is: {}'.format(count_token_average_train(i, json_objects_train)))\n",
    "\n",
    "\n",
    "## test\n",
    "def count_token_average_test(label, dataset):\n",
    "    average_num = 0\n",
    "    docu_num = test_classes_count.get(label)\n",
    "    for i in range(len(dataset)):\n",
    "        json_data = json.loads(dataset[i])\n",
    "        classes = json_data.get('label')\n",
    "        text = json_data.get('text')\n",
    "        if classes == label:\n",
    "            token = nltk.word_tokenize(text)\n",
    "            average_num += len(token) / docu_num\n",
    "            \n",
    "    return average_num\n",
    "\n",
    "print(\"In test data:\")\n",
    "for i in set(classes_test):\n",
    "    print('average number of tokens in', i, 'is: {}'.format(count_token_average_test(i, json_objects_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d999a66-977c-469b-939c-c3934210972e",
   "metadata": {},
   "source": [
    "> 4) For each sentence in the document, remove punctuations and other special characters so that each sentence only contains English words and numbers. To make your life easier, you can make all words as lower cases. For each class, print out the first article's name and the processed first 40 words. (for both train and test dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "819a28ef-0821-4ace-be70-fb9b249d355b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In train dataset:\n",
      "Citizen_Kane : Citizen Kane is a 9 American drama film produced by directed by and starring Orson Welles He also co-wrote the screenplay with Herman J Mankiewicz The picture was Welles first feature film Citizen Kane is considered by many critics and\n",
      "Roman_Polanski : Roman Polanski born Raymond Thierry Liebling on August 9 is a Polish French film director producer screenwriter and actor His Polish Jewish parents moved the family from Paris back to Krak w in 9 Two years later Poland was invaded\n",
      "Mircea_Eliade : Mircea Eliade April 9 was a Romanian historian of religion fiction writer philosopher and professor at the University of Chicago He was a leading interpreter of religious experience who established paradigms in religious studies that persist to this day His\n",
      "Domestic_violence : Domestic violence also called domestic abuse or family violence is violence or other abuse that occurs in a domestic setting such as in a marriage or cohabitation Domestic violence is often used as a synonym for intimate partner violence which\n",
      "Korean_cuisine : Korean cuisine has evolved through centuries of social and political change Originating from ancient agricultural and nomadic traditions in Korea and southern Manchuria Korean cuisine reflects a complex interaction of the natural environment and different cultural trends Korean cuisine is\n",
      "Oesophagostomum : Oesophagostomum is a genus of parasitic nematodes roundworms of the family Strongylidae These worms occur in Africa Brazil China Indonesia and the Philippines The majority of human infection with Oesophagostomum is localized to northern Togo and Ghana Because the eggs\n",
      "Android_(operating_system) : Android is a mobile operating system based on a modified version of the Linux kernel and other open source software designed primarily for touchscreen mobile devices such as smartphones and tablets Android is developed by a consortium of developers known\n",
      "Charles_de_Gaulle : Charles Andr Joseph Marie de Gaulle November 9 9 November 9 was a French army officer and statesman who led Free France against Nazi Germany in World War II and chaired the Provisional Government of the French Republic from 9\n",
      "Mihai_Olos : Mihai Olos born February 9 in Arini Romania died February in Amoltern Endigen Germany was a Romanian conceptual artist poet essayist A gifted colorist in his first paintings he became more attracted towards experimenting with various forms and materials Familiar\n",
      "The_Spirit_of_the_Age : The Spirit of the Age full title The Spirit of the Age Or Contemporary Portraits is a collection of character sketches by the early 9th century English essayist literary critic and social commentator William Hazlitt portraying men mostly British whom\n",
      "In test dataset:\n",
      "Monty_Python's_Life_of_Brian : Monty Python s Life of Brian also known as Life of Brian is a 9 9 British comedy film starring and written by the comedy group Monty Python Graham Chapman John Cleese Terry Gilliam Eric Idle Terry Jones and Michael\n",
      "Kom_Chuanchuen : Akom Preedakul January 9 April known by stage name Kom Chuanchuen was a Thai comedian and actor best known from comedic supporting roles in Thai movies and television Originally a nightclub comedian Kom made a breakthrough in movies after starring\n",
      "Horia_Gârbea : Horia-R zvan G rbea or G rbea born August 9 is a Romanian playwright poet essayist novelist and critic also known as an academic engineer and journalist Known for his work in experimental theater and his Postmodernist contributions to Romanian\n",
      "Staphylococcus_aureus : Staphylococcus aureus is a Gram-positive round-shaped bacterium a member of the Firmicutes and is a usual member of the microbiota of the body frequently found in the upper respiratory tract and on the skin It is often positive for catalase\n",
      "Sponge_cake : Sponge cake is a light cake made with egg whites flour and sugar sometimes leavened with baking powder Sponge cakes leavened with beaten eggs originated during the Renaissance possibly in Spain The sponge cake is thought to be one of\n",
      "Articulata_hypothesis : The Articulata hypothesis is the grouping in a higher taxon of animals with segmented bodies consisting of Annelida and Panarthropoda This theory states that these groups are descended from a common segmented ancestor The Articulata hypothesis is an alternative to\n",
      "Unix : Unix trademarked as UNIX is a family of multitasking multiuser computer operating systems that derive from the original AT amp T Unix whose development started in 9 9 at the Bell Labs research center by Ken Thompson Dennis Ritchie and\n",
      "Olusegun_Obasanjo : Chief Olusegun Matthew Okikiola Aremu Obasanjo GCFR born March 9 is a Nigerian political and military leader who served as Nigeria s head of state from 9 to 9 9 and later as its president from 999 to Ideologically a\n",
      "Camille_Pissarro : Camille Pissarro July November 9 was a Danish-French Impressionist and Neo-Impressionist painter born on the island of St Thomas now in the US Virgin Islands but then in the Danish West Indies His importance resides in his contributions to both\n",
      "Cousin_Bette : La Cousine Bette Cousin Bette is an novel by French author Honor de Balzac Set in mid- 9th-century Paris it tells the story of an unmarried middle-aged woman who plots the destruction of her extended family Bette works with Val\n"
     ]
    }
   ],
   "source": [
    "# Your code goes to here\n",
    "import nltk\n",
    "def remove_punc_and_others(dataset):\n",
    "    pattern = r'[a-zA-Z-9]+'\n",
    "    for i in range(len(dataset)):\n",
    "        json_data = json.loads(dataset[i])\n",
    "        text = json_data.get(\"text\")\n",
    "        words = nltk.regexp_tokenize(text, pattern)\n",
    "        cleaned_text = ' '.join(word for word in words)\n",
    "        print(cleaned_text)\n",
    "\n",
    "## train       \n",
    "def print_fun(label, dataset):\n",
    "    pattern = r'[a-zA-Z-9]+'\n",
    "    for i in range(len(dataset)):\n",
    "        json_data = json.loads(dataset[i])\n",
    "        classes = json_data.get('label')\n",
    "        text = json_data.get(\"text\")\n",
    "        title = json_data.get(\"title\")\n",
    "        if classes == label:\n",
    "            words = nltk.regexp_tokenize(text, pattern)\n",
    "            cleaned_text = ' '.join(word for word in words[:40])\n",
    "            print(title,':', cleaned_text)\n",
    "            break\n",
    "\n",
    "\n",
    "print(\"In train dataset:\")\n",
    "for i in set(classes_train):\n",
    "    print_fun(i, json_objects_train)\n",
    "\n",
    "## test\n",
    "print(\"In test dataset:\")\n",
    "for i in set(classes_test):\n",
    "    print_fun(i, json_objects_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6c7ea9-18fa-43f1-bded-625c09a947b1",
   "metadata": {},
   "source": [
    "## Task2 - Build language models\n",
    "\n",
    "> 1) Based on the training dataset, build unigram, bigram, and trigram language models using Add-one smoothing technique. It is encouraged to implement models by yourself. If you use public code, please cite it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b86b26",
   "metadata": {},
   "source": [
    "在理解清楚https://github.com/joshualoehr/ngram-language-model 的 （即老师在demo code L02里提供的）代码的前提下，按照他大致的整体结构编写而成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "69c59098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes to here\n",
    "## 代码的整体结构学习 https://github.com/joshualoehr/ngram-language-model 的\n",
    "import nltk\n",
    "from itertools import product\n",
    "import math\n",
    "\n",
    "def token_text(dataset, n, k):\n",
    "    sos = \"<s> \" * (n-1) if n > 1 else \"<s> \"\n",
    "    tokenized_word = []\n",
    "    for i in range(len(dataset)):\n",
    "        obj = json.loads(dataset[i])\n",
    "        text = obj.get(\"text\")\n",
    "        text = text.replace(\"\\n\", \" \")\n",
    "        text_split = nltk.sent_tokenize(text)\n",
    "        tokenized_sentences = ['{}{} {}'.format(sos, sent, \"</s>\").split() for sent in text_split]\n",
    "        tokens = [token for sublist in tokenized_sentences for token in sublist]\n",
    "        tokenized_word.extend(tokens)\n",
    "    vocab = nltk.FreqDist(tokenized_word)\n",
    "    tokenized_word = [token if vocab[token] > k else \"<UNK>\" for token in tokenized_word]\n",
    "    return tokenized_word, tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd2219a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel():\n",
    "    def __init__(self, tokens, n):\n",
    "        self.tokens = tokens\n",
    "        self.n = n\n",
    "        self.vocab  = nltk.FreqDist(self.tokens)\n",
    "        self.vocabulary_size = len(set(self.tokens)) \n",
    "        self.model = self.train_model()\n",
    "        self.mask = list(reversed(list(product((0,1), repeat=n))))\n",
    "    def train_model(self):\n",
    "        \"\"\"建立模型\n",
    "        \n",
    "        Returns:\n",
    "            N-gram概率\n",
    "        \"\"\"\n",
    "        if self.n == 1:\n",
    "            num_tokens = len(self.tokens)\n",
    "            # n_grams = nltk.ngrams(self.tokens, self.n)\n",
    "            return { (unigram,): (count + 1) /(num_tokens + 1 * self.vocabulary_size) for unigram, count in self.vocab.items() }\n",
    "        else:\n",
    "            n_grams = nltk.ngrams(self.tokens, self.n)\n",
    "            n_vocab = nltk.FreqDist(n_grams)\n",
    "\n",
    "            m_grams = nltk.ngrams(self.tokens, self.n-1)\n",
    "            m_vocab = nltk.FreqDist(m_grams)\n",
    "            def smoothed_count(n_gram, n_count):\n",
    "                m_gram = n_gram[:-1]\n",
    "                m_count = m_vocab[m_gram]\n",
    "                return (n_count + 1) / (m_count + 1 * self.vocabulary_size)\n",
    "            return { n_gram: smoothed_count(n_gram, count) for n_gram, count in n_vocab.items() }\n",
    "\n",
    "    def change_testtoken(self, ngram):\n",
    "        \"\"\"_summary_\n",
    "            没有出现的字词换为<NUK>\n",
    "        \"\"\"\n",
    "        mask = lambda ngram, bitmask: tuple((token if flag == 1 else \"<UNK>\" for token,flag in zip(ngram, bitmask)))\n",
    "\n",
    "        ngram = (ngram,) if type(ngram) is str else ngram\n",
    "        for possible_known in [mask(ngram, bitmask) for bitmask in self.mask]:\n",
    "            if possible_known in self.model:\n",
    "                return possible_known\n",
    "\n",
    "            \n",
    "    def perplexity(self, test_tokens):\n",
    "        test_ngrams = nltk.ngrams(test_tokens, self.n)\n",
    "        N = len(test_tokens)\n",
    "        known_ngrams  = (self.change_testtoken(ngram) for ngram in test_ngrams)\n",
    "        probabilities = [self.model[ngram] for ngram in known_ngrams]\n",
    "\n",
    "        return math.exp((-1/N) * sum(map(math.log, probabilities)))\n",
    "    \n",
    "    def generate_sentences(self, num, min_len = 12, max_len = 24):\n",
    "        def max_prob_word(prev, i, without=[]):\n",
    "            blacklist = [\"<UNK>\"] + without\n",
    "            candidates = ((ngram[-1],prob) for ngram, prob in self.model.items() if ngram[:-1]== prev)\n",
    "            candidates = [candidate for candidate in candidates if candidate[0] not in blacklist]\n",
    "            word_list = sorted(candidates, key=lambda candidate: candidate[1], reverse=True)\n",
    "            if len(word_list) == 0:\n",
    "                return (\"</s>\", 1)\n",
    "            elif prev != () and prev[-1] != \"<s>\":\n",
    "                return word_list[0]\n",
    "        \n",
    "        for i in range(num):\n",
    "            sent, prob = [\"<s>\"] * max(1, self.n-1), 1\n",
    "            while sent[-1] != \"</s>\":\n",
    "                if self.n == 1:\n",
    "                    prev = () \n",
    "                else:\n",
    "                    prev = tuple(sent[-(self.n-1):])\n",
    "                    \n",
    "                blacklist = sent + ([\"</s>\"] if len(sent) < min_len else [])\n",
    "                next_token, next_prob = max_prob_word(prev, i, without=blacklist)\n",
    "                sent.append(next_token)\n",
    "                prob *= next_prob\n",
    "                \n",
    "                if len(sent) >= max_len:\n",
    "                    sent.append(\"</s>\")\n",
    "\n",
    "            print( ' '.join(sent))\n",
    "            print(\"句子的概率是：\", -1/math.log(prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "66eb054c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 模型的建立\n",
    "class LanguageModel():\n",
    "    def __init__(self, tokens, n):\n",
    "        self.tokens = tokens\n",
    "        self.n = n\n",
    "        self.vocab  = nltk.FreqDist(self.tokens)\n",
    "        self.vocabulary_size = len(set(self.tokens)) \n",
    "        self.model = self.train_model()\n",
    "        self.mask = list(reversed(list(product((0,1), repeat=n))))\n",
    "        \n",
    "    def train_model(self):\n",
    "        \"\"\"建立模型\n",
    "        \n",
    "        Returns:\n",
    "            N-gram概率\n",
    "        \"\"\"\n",
    "        if self.n == 1:\n",
    "            num_tokens = len(self.tokens)\n",
    "            # n_grams = nltk.ngrams(self.tokens, self.n)\n",
    "            return { (unigram,): (count + 1) /(num_tokens + 1*self.vocabulary_size) for unigram, count in self.vocab.items() }\n",
    "        else:\n",
    "            n_grams = nltk.ngrams(self.tokens, self.n)\n",
    "            n_vocab = nltk.FreqDist(n_grams)\n",
    "\n",
    "            m_grams = nltk.ngrams(self.tokens, self.n-1)\n",
    "            m_vocab = nltk.FreqDist(m_grams)\n",
    "            def smoothed_count(n_gram, n_count):\n",
    "                m_gram = n_gram[:-1]\n",
    "                m_count = m_vocab[m_gram]\n",
    "                return (n_count + 1) / (m_count + 1*self.vocabulary_size)\n",
    "            return { n_gram: smoothed_count(n_gram, count) for n_gram, count in n_vocab.items() }\n",
    "\n",
    "    def change_testtoken(self, ngram):\n",
    "        \"\"\"_summary_\n",
    "            没有出现的字词换为<NUK>\n",
    "        \"\"\"\n",
    "        mask = lambda ngram, bitmask: tuple((token if flag == 1 else \"<UNK>\" for token,flag in zip(ngram, bitmask)))\n",
    "\n",
    "        ngram = (ngram,) if type(ngram) is str else ngram\n",
    "        for possible_known in [mask(ngram, bitmask) for bitmask in self.mask]:\n",
    "            if possible_known in self.model:\n",
    "                return possible_known\n",
    "            \n",
    "    def perplexity(self, test_tokens):\n",
    "        test_ngrams = nltk.ngrams(test_tokens, self.n)\n",
    "        N = len(test_tokens)\n",
    "        known_ngrams  = (self.change_testtoken(ngram) for ngram in test_ngrams)\n",
    "        probabilities = [self.model[ngram] for ngram in known_ngrams]\n",
    "\n",
    "        return math.exp((-1/N) * sum(map(math.log, probabilities)))\n",
    "    \n",
    "    def generate_sentences(self, num, min_len = 12, max_len = 24):\n",
    "        def max_prob_word(prev, i, without=[]):\n",
    "            blacklist = [\"<UNK>\"] + without\n",
    "            candidates = ((ngram[-1],prob) for ngram,prob in self.model.items() if ngram[:-1]==prev)\n",
    "            candidates = filter(lambda candidate: candidate[0] not in blacklist, candidates)\n",
    "            word_list = sorted(candidates, key=lambda candidate: candidate[1], reverse=True)\n",
    "            if len(word_list) == 0:\n",
    "                return (\"</s>\", 1)\n",
    "            else:\n",
    "                return word_list[0 if prev != () and prev[-1] != \"<s>\" else i]\n",
    "        \n",
    "        for i in range(num):\n",
    "            sent, prob = [\"<s>\"] * max(1, self.n-1), 1\n",
    "            while sent[-1] != \"</s>\":\n",
    "                prev = () if self.n == 1 else tuple(sent[-(self.n-1):])\n",
    "                blacklist = sent + ([\"</s>\"] if len(sent) < min_len else [])\n",
    "                next_token, next_prob = max_prob_word(prev, i, without=blacklist)\n",
    "                sent.append(next_token)\n",
    "                prob *= next_prob\n",
    "                \n",
    "                if len(sent) >= max_len:\n",
    "                    sent.append(\"</s>\")\n",
    "\n",
    "            yield ' '.join(sent), -math.log10(prob)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb6edc5-78c5-4de0-8855-2a455d625c97",
   "metadata": {},
   "source": [
    "> 2) Report the perplexity of these 3 trained models on the testing dataset and explain your findings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3acda9a7-606c-4fa6-918a-9ebe7745ad42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Loading 1-gram model...\n",
      "Model perplexity: 1172.520\n",
      "--------------------------------------------------\n",
      "Loading 2-gram model...\n",
      "Model perplexity: 1906.412\n",
      "--------------------------------------------------\n",
      "Loading 3-gram model...\n",
      "Model perplexity: 4956.581\n"
     ]
    }
   ],
   "source": [
    "# Your code goes to here\n",
    "\n",
    "for n in [1,2,3]:\n",
    "    print(\"-\" * 50)\n",
    "    train_tokens, train_sentences = token_text(json_objects_train, n, 1)\n",
    "    test_tokens, test_sentences = token_text(json_objects_test, n, 1)\n",
    "    print(\"Loading {}-gram model...\".format(n))\n",
    "    lm = LanguageModel(train_tokens, n)\n",
    "    perplexity = lm.perplexity(test_tokens)\n",
    "    print(\"Model perplexity: {:.3f}\".format(perplexity))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedaebf9",
   "metadata": {},
   "source": [
    "可以看出模型并不是很好(perplexity稳步增加)\n",
    "\n",
    "一般来说，应该2-gram的效果比1-gram要好，而3-gram的效果可能比2-gram略差（因为训练数据规模可能不够大）但还是应该比1-gram要好\n",
    "\n",
    "说明可能模型某些超参数设置不是特别何理，我们将在第四部分讨论这个内容"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd75b35-134d-4629-ac87-ed84fcc9d8e4",
   "metadata": {},
   "source": [
    "> 3) Use each built model to generate five sentences and explain these generated patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ea6e1244-2b15-4845-a83b-5708797569b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------- 1-gram -------------------------\n",
      "Loading 1-gram model...\n",
      "Generating sentences...\n",
      "<s> the of and to in a was that as for with </s> \n",
      " 句子的-log10(概率)是(21.31607)\n",
      "<s> of and to in a was that as for with is </s> \n",
      " 句子的-log10(概率)是(22.17082)\n",
      "<s> and to in a was that as for with is his of by on The he from In an be at had which </s> \n",
      " 句子的-log10(概率)是(49.27788)\n",
      "<s> to in a was that as for with is his by and on The he from In an be at had which it </s> \n",
      " 句子的-log10(概率)是(50.27780)\n",
      "<s> in a was that as for with is his by on to The he from In an be at had which it are </s> \n",
      " 句子的-log10(概率)是(51.22885)\n"
     ]
    }
   ],
   "source": [
    "# Your code goes to here\n",
    "## 对于1-gram\n",
    "num_sent_gen = 5\n",
    "print(\"-\" * 25, \"1-gram\", \"-\"*25)\n",
    "print(\"Loading {}-gram model...\".format(1))\n",
    "train_tokens, train_sentences = token_text(json_objects_train, 1, 1)\n",
    "lm1 = LanguageModel(train_tokens, 1)\n",
    "print(\"Generating sentences...\")\n",
    "for sentence, prob in lm1.generate_sentences(num_sent_gen):\n",
    "    print(\"{} \\n 句子的-log10(概率)是({:.5f})\".format(sentence, prob))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83114ec4",
   "metadata": {},
   "source": [
    "我们可以看到：\n",
    "\n",
    "1. 生成的句子包含了一些常见的高频词汇，如\"the\"、\"of\"、\"and\"、\"to\"等，说明这些词汇在1-gram模型的语料库中出现频率很高。（一会验证）\n",
    "\n",
    "2. 由于1-gram模型只考虑单个词的概率，因此生成的句子可能会缺乏语法结构和逻辑连贯性。例如，句子中可能会出现一些语法错误。\n",
    "\n",
    "3. 生成的句子长度较短，通常由几个常见的词汇组成。\n",
    "\n",
    "总的来说，1-gram模型生成的句子通常简单、短小，并且缺乏上下文的连贯性和语法结构。这是因为1-gram模型只考虑单个词的概率，而没有考虑词与词之间的关联性。因此，生成的句子可能会显得不够自然和连贯。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "34a86def",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('the',), 0.04979984708618549),\n",
       " (('<s>',), 0.04107336462852824),\n",
       " (('</s>',), 0.04107336462852824),\n",
       " (('of',), 0.03044493159681271),\n",
       " (('<UNK>',), 0.029501242546180168),\n",
       " (('and',), 0.027003277834880282),\n",
       " (('to',), 0.02205373745067218),\n",
       " (('in',), 0.01964199320339869)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(lm1.model.items(), key= lambda token:token[1], reverse=True)[:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317c6bd5",
   "metadata": {},
   "source": [
    "确实是一些介词连词代词概率大"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "04f4aeaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------- 2-gram -------------------------\n",
      "Loading 2-gram model...\n",
      "Generating sentences...\n",
      "<s> The film was a new version of the first time in his own and that he had been found to be used for </s> \n",
      " 句子的-log10(概率)是(49.54975)\n",
      "<s> In the first time of a new version was not be used to his own and that he had been found in which </s> \n",
      " 句子的-log10(概率)是(51.36339)\n",
      "<s> He was a new version of the first time in his own and that he had been found to be used for its </s> \n",
      " 句子的-log10(概率)是(49.95609)\n",
      "<s> It is a new version of the first time in his own and was not be used to have been found that he </s> \n",
      " 句子的-log10(概率)是(49.96390)\n",
      "<s> This is a new version of the first time in his own and was not be used to have been found that he </s> \n",
      " 句子的-log10(概率)是(50.51037)\n"
     ]
    }
   ],
   "source": [
    "## 对于2-gram\n",
    "num_sent_gen = 5\n",
    "print(\"-\" * 25, \"2-gram\", \"-\"*25)\n",
    "print(\"Loading {}-gram model...\".format(2))\n",
    "train_tokens, train_sentences = token_text(json_objects_train, 2, 1)\n",
    "lm2 = LanguageModel(train_tokens, 2)\n",
    "print(\"Generating sentences...\")\n",
    "for sentence, prob in lm2.generate_sentences(num_sent_gen):\n",
    "    print(\"{} \\n 句子的-log10(概率)是({:.5f})\".format(sentence, prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b5d4ff",
   "metadata": {},
   "source": [
    "我们可以看出：\n",
    "1. 生成的句子相比1-gram模型更具有上下文连贯性和语义合理性，因为2-gram模型考虑了相邻两个词之间的关联性。\n",
    "\n",
    "2. 由于2-gram模型考虑了相邻两个词之间的关联性，生成的句子中会出现一些常见的词组和短语，开始出现除了介词连词助词以外的实意词。\n",
    "\n",
    "3. 生成的句子长度适中，通常由几个常见的词组组成，但仍然受到局部上下文的限制。\n",
    "\n",
    "4. 由于2-gram模型只考虑相邻两个词之间的关联性，生成的句子可能会出现一些语义不连贯或语法不对的地方。\n",
    "\n",
    "总的来说，2-gram模型生成的句子相比1-gram模型更具有上下文连贯性和语义合理性，但仍然可能存在一些局部的语法错误或不连贯之处。这是因为2-gram模型考虑了相邻两个词之间的关联性，但仍然受到局部上下文的限制。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d7109b5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('</s>', '<s>'), 0.6631479263025188),\n",
       " (('of', 'the'), 0.14265731617551342),\n",
       " (('in', 'the'), 0.11167441163243891),\n",
       " (('<UNK>', '</s>'), 0.1066721641132474),\n",
       " (('<s>', 'The'), 0.08030492264195706),\n",
       " (('to', 'the'), 0.06194222008432133),\n",
       " (('on', 'the'), 0.058464159779865334),\n",
       " (('<s>', 'In'), 0.05654950385368732)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(lm2.model.items(), key= lambda token:token[1], reverse=True)[:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e21183b",
   "metadata": {},
   "source": [
    "可以看出概率在前面的还是介词连词动词的组合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e2f5d6dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------- 3-gram -------------------------\n",
      "Loading 3-gram model...\n",
      "Generating sentences...\n",
      "<s> <s> The film was released on October 31, 2014. </s> \n",
      " 句子的-log10(概率)是(23.57530)\n",
      "<s> <s> In the United States and Canada, it is not a single large room which was released on October 31, 2014. </s> \n",
      " 句子的-log10(概率)是(73.40615)\n",
      "<s> <s> He was also a member of the most common cause with </s> \n",
      " 句子的-log10(概率)是(32.02132)\n",
      "<s> <s> It is a major role in the United States and Canada, it had been an important part of his own work on </s> \n",
      " 句子的-log10(概率)是(72.01382)\n",
      "<s> <s> This is the most common cause of death in a letter to his own work on \"The Lord </s> \n",
      " 句子的-log10(概率)是(60.84203)\n"
     ]
    }
   ],
   "source": [
    "## 对于3-gram\n",
    "num_sent_gen = 5\n",
    "print(\"-\" * 25, \"3-gram\", \"-\"*25)\n",
    "print(\"Loading {}-gram model...\".format(3))\n",
    "train_tokens, train_sentences = token_text(json_objects_train, 3, 1)\n",
    "lm3 = LanguageModel(train_tokens, 3)\n",
    "print(\"Generating sentences...\")\n",
    "for sentence, prob in lm3.generate_sentences(num_sent_gen):\n",
    "    print(\"{} \\n 句子的-log10(概率)是({:.5f})\".format(sentence, prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4abab2f",
   "metadata": {},
   "source": [
    "我们可以看到：\n",
    "1. 3-gram有了合理的上下文关联：由于考虑了前两个单词的影响，生成的句子会更符合自然语言的语境，单词之间的关联性更强，句子更具有连贯性。\n",
    "2. 生成的句子中常见短语和结构。\n",
    "3. 句子有了更好的语法和语义：相比于2-gram模型，3-gram模型能够更好地捕捉到单词之间的语法和语义关系，生成的句子在语法和语义上更加合理。\n",
    "4. 可以生成一个符合语法有语义的句子，如第一句话\n",
    "   \n",
    "总的来说，生成更好的句子的原因在于3-gram模型考虑了更多的上下文信息，能够更好地预测下一个单词出现的概率，从而生成更加合理的句子。通过考虑更多的上下文信息，3-gram模型能够更好地捕捉到单词之间的关联性，生成更接近自然语言的句子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d0da39e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('</s>', '<s>', '<s>'), 0.6631492265160873),\n",
       " (('<UNK>', '</s>', '<s>'), 0.20477678261473303),\n",
       " (('<s>', '<s>', 'The'), 0.08030492264195706),\n",
       " (('<s>', '<s>', 'In'), 0.05654950385368732),\n",
       " (('<s>', '<s>', 'He'), 0.029157051238062),\n",
       " (('<s>', '<s>', '<UNK>'), 0.02014975560309079),\n",
       " (('<s>', 'In', 'the'), 0.02012001236007695),\n",
       " (('<UNK>', '<UNK>', '<UNK>'), 0.019895673455842703),\n",
       " (('and', '<UNK>', '</s>'), 0.017530110318328518),\n",
       " (('as', 'well', 'as'), 0.01658324514486743)]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(lm3.model.items(), key= lambda token:token[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24522cf5",
   "metadata": {},
   "source": [
    "看起来未知的单词概率更大\n",
    "也有as, well, as 这样的短语"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c127fcc4",
   "metadata": {},
   "source": [
    "> 4) 模型改良（不用laplace = 1 及增大UNK数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9b5669dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when freq <= 2 turn into <UNK> , Loading 1-gram model... Model perplexity: 833.608\n",
      "--------------------------------------------------\n",
      "when freq <= 2 turn into <UNK> , Loading 2-gram model... Model perplexity: 1160.163\n",
      "--------------------------------------------------\n",
      "when freq <= 2 turn into <UNK> , Loading 3-gram model... Model perplexity: 3260.697\n",
      "--------------------------------------------------\n",
      "when freq <= 4 turn into <UNK> , Loading 1-gram model... Model perplexity: 543.978\n",
      "--------------------------------------------------\n",
      "when freq <= 4 turn into <UNK> , Loading 2-gram model... Model perplexity: 629.916\n",
      "--------------------------------------------------\n",
      "when freq <= 4 turn into <UNK> , Loading 3-gram model... Model perplexity: 1891.847\n",
      "--------------------------------------------------\n",
      "when freq <= 6 turn into <UNK> , Loading 1-gram model... Model perplexity: 405.606\n",
      "--------------------------------------------------\n",
      "when freq <= 6 turn into <UNK> , Loading 2-gram model... Model perplexity: 422.236\n",
      "--------------------------------------------------\n",
      "when freq <= 6 turn into <UNK> , Loading 3-gram model... Model perplexity: 1304.012\n",
      "--------------------------------------------------\n",
      "when freq <= 10 turn into <UNK> , Loading 1-gram model... Model perplexity: 266.500\n",
      "--------------------------------------------------\n",
      "when freq <= 10 turn into <UNK> , Loading 2-gram model... Model perplexity: 242.564\n",
      "--------------------------------------------------\n",
      "when freq <= 10 turn into <UNK> , Loading 3-gram model... Model perplexity: 754.650\n",
      "--------------------------------------------------\n",
      "when freq <= 20 turn into <UNK> , Loading 1-gram model... Model perplexity: 145.985\n",
      "--------------------------------------------------\n",
      "when freq <= 20 turn into <UNK> , Loading 2-gram model... Model perplexity: 112.453\n",
      "--------------------------------------------------\n",
      "when freq <= 20 turn into <UNK> , Loading 3-gram model... Model perplexity: 316.916\n"
     ]
    }
   ],
   "source": [
    "for k in [2, 4, 6, 10, 20]:\n",
    "    for n in [1,2,3]:\n",
    "        print(\"-\" * 50)\n",
    "        train_tokens, train_sentences = token_text(json_objects_train, n, k)\n",
    "        test_tokens, test_sentences = token_text(json_objects_test, n, k)\n",
    "        # print(\"Loading {}-gram model...\".format(n))\n",
    "        lm = LanguageModel(train_tokens, n)\n",
    "        perplexity = lm.perplexity(test_tokens)\n",
    "        print(\"when freq <= {} turn into <UNK> , Loading {}-gram model...\".format(k, n), \"Model perplexity: {:.3f}\".format(perplexity))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf83b3c",
   "metadata": {},
   "source": [
    "可以看出，随着设置的UNK阈值的增大，模型的困惑度下降， 可能是由于以下原因：\n",
    "\n",
    "1. 减少罕见词汇的干扰：更多的低频词被标记为UNK，这样模型就不用太过关注那些不常见的词汇，更专注于学习常见词汇的规律，从而降低困惑度。\n",
    "\n",
    "2. 简化处理：减少词汇表的大小让模型更轻松地学习和泛化，不用处理太多生僻词汇，这有助于提升模型的表现。\n",
    "\n",
    "3. 更好的适应性：增大UNK阈值让模型更多地把低频词汇看作同一类未知词，这有助于模型更好地适应处理新领域或不常见词汇，提升泛化能力。\n",
    "\n",
    "总的来说，调整UNK阈值可以让模型更宽泛地处理文本，不被罕见词汇影响，表现得更好。\n",
    "\n",
    "但是UNK阈值太高，可能会造成生成的句子不好，语言模型太宽泛\n",
    "\n",
    "由于1-gram生成出来都是介词，在这里我们省略其生成，只看2-gram和3-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7c858afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "when freq <= 2 turn into <UNK> , Loading 2-gram model...\n",
      "<s> The film was a new version of the first time in his own and that he had been found to be used for </s> \n",
      " 句子的-log10(概率)是(46.79387)\n",
      "<s> In the first time of a new version was not be used to his own and that he had been found in which </s> \n",
      " 句子的-log10(概率)是(48.60860)\n",
      "<s> He was a new version of the first time in his own and that he had been found to be used for its </s> \n",
      " 句子的-log10(概率)是(47.21784)\n",
      "--------------------------------------------------\n",
      "when freq <= 2 turn into <UNK> , Loading 3-gram model...\n",
      "<s> <s> The film was released on October 31, 2014. </s> \n",
      " 句子的-log10(概率)是(22.40032)\n",
      "<s> <s> In the United States and Canada, it is not a single large room which was released on October 31, 2014. </s> \n",
      " 句子的-log10(概率)是(70.07187)\n",
      "<s> <s> He was also a member of the most common cause with </s> \n",
      " 句子的-log10(概率)是(30.37858)\n",
      "--------------------------------------------------\n",
      "when freq <= 4 turn into <UNK> , Loading 2-gram model...\n",
      "<s> The film was a new version of the first time in his own and that he had been found to be used for </s> \n",
      " 句子的-log10(概率)是(43.97238)\n",
      "<s> In the first time of a new version was not be used to his own and that he had been found in which </s> \n",
      " 句子的-log10(概率)是(45.79023)\n",
      "<s> He was a new version of the first time in his own and that he had been found to be used for its </s> \n",
      " 句子的-log10(概率)是(44.42050)\n",
      "--------------------------------------------------\n",
      "when freq <= 4 turn into <UNK> , Loading 3-gram model...\n",
      "<s> <s> The film was released on October 31, 2014. </s> \n",
      " 句子的-log10(概率)是(21.11864)\n",
      "<s> <s> In the United States and Canada, it is not a single large room which was released on October 31, 2014. </s> \n",
      " 句子的-log10(概率)是(66.38427)\n",
      "<s> <s> He was also a member of the most common cause with </s> \n",
      " 句子的-log10(概率)是(28.58662)\n",
      "--------------------------------------------------\n",
      "when freq <= 6 turn into <UNK> , Loading 2-gram model...\n",
      "<s> The film was a new version of the first time in his own and that he had been found to be used for </s> \n",
      " 句子的-log10(概率)是(42.38858)\n",
      "<s> In the first time of a new version was not be used to his own and that he had been found in which </s> \n",
      " 句子的-log10(概率)是(44.20991)\n",
      "<s> He was a new version of the first time in his own and that he had been found to be used for its </s> \n",
      " 句子的-log10(概率)是(42.85358)\n",
      "--------------------------------------------------\n",
      "when freq <= 6 turn into <UNK> , Loading 3-gram model...\n",
      "<s> <s> The film was released on October 31, 2014. </s> \n",
      " 句子的-log10(概率)是(20.35210)\n",
      "<s> <s> In the United States and Canada, it is not a single large room which was released on October 31, 2014. </s> \n",
      " 句子的-log10(概率)是(64.15332)\n",
      "<s> <s> He was also a member of the most common cause with </s> \n",
      " 句子的-log10(概率)是(27.51598)\n",
      "--------------------------------------------------\n",
      "when freq <= 10 turn into <UNK> , Loading 2-gram model...\n",
      "<s> The film was a new version of the first time in his own and that he had been found to be used for </s> \n",
      " 句子的-log10(概率)是(40.47706)\n",
      "<s> In the first time of a new version was not be used to his own and that he had been found in which </s> \n",
      " 句子的-log10(概率)是(42.30538)\n",
      "<s> He was a new version of the first time in his own and that he had been found to be used for its </s> \n",
      " 句子的-log10(概率)是(40.96613)\n",
      "--------------------------------------------------\n",
      "when freq <= 10 turn into <UNK> , Loading 3-gram model...\n",
      "<s> <s> The film was released on October 31, 2014. </s> \n",
      " 句子的-log10(概率)是(19.36618)\n",
      "<s> <s> In the United States and Canada, it is not a single large room which was released on October 31, 2014. </s> \n",
      " 句子的-log10(概率)是(61.25671)\n",
      "<s> <s> He was also a member of the most common cause with </s> \n",
      " 句子的-log10(概率)是(26.14167)\n",
      "--------------------------------------------------\n",
      "when freq <= 20 turn into <UNK> , Loading 2-gram model...\n",
      "<s> The film was a new version of the first time in his own and that he had been found to be used for </s> \n",
      " 句子的-log10(概率)是(38.11490)\n",
      "<s> In the first time of a new version was not be used to his own and that he had been found in which </s> \n",
      " 句子的-log10(概率)是(39.95861)\n",
      "<s> He was a new version of the first time in his own and that he had been found to be used for its </s> \n",
      " 句子的-log10(概率)是(38.64007)\n",
      "--------------------------------------------------\n",
      "when freq <= 20 turn into <UNK> , Loading 3-gram model...\n",
      "<s> <s> The film was released on October 31, 2014. </s> \n",
      " 句子的-log10(概率)是(18.01986)\n",
      "<s> <s> In the United States and Canada, it is not a single large room which was released on October 31, 2014. </s> \n",
      " 句子的-log10(概率)是(57.25657)\n",
      "<s> <s> He was also a member of the most common cause with </s> \n",
      " 句子的-log10(概率)是(24.27369)\n"
     ]
    }
   ],
   "source": [
    "num_sent_test = 3\n",
    "for k in [2, 4, 6, 10, 20]:\n",
    "    for n in [2,3]:\n",
    "        print(\"-\" * 50)\n",
    "        train_tokens, train_sentences = token_text(json_objects_train, n, k)\n",
    "        test_tokens, test_sentences = token_text(json_objects_test, n, k)\n",
    "        print(\"when freq <= {} turn into <UNK> , Loading {}-gram model...\".format(k, n))\n",
    "        lm = LanguageModel(train_tokens, n)\n",
    "        for sentence, prob in lm.generate_sentences(num_sent_test):\n",
    "            print(\"{} \\n 句子的-log10(概率)是({:.5f})\".format(sentence, prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab02c40",
   "metadata": {},
   "source": [
    "嘶可能生成的句子太少了 有点区分不出来\n",
    "\n",
    "我们居中挑选了6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a408fd1",
   "metadata": {},
   "source": [
    "我们还可以更改laplace值，原代码中设置了1， 我们可以改成2，4， 0.1， 0.01 看看效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "10cef4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 模型的建立\n",
    "class LanguageModel_2(object):\n",
    "    def __init__(self, tokens, n, laplace):\n",
    "        self.tokens = tokens\n",
    "        self.n = n\n",
    "        self.laplace = laplace\n",
    "        self.vocab  = nltk.FreqDist(self.tokens)\n",
    "        self.vocabulary_size = len(set(self.tokens)) \n",
    "        self.model = self.train_model()\n",
    "        self.mask = list(reversed(list(product((0,1), repeat=n))))\n",
    "        \n",
    "    def train_model(self):\n",
    "        \"\"\"建立模型\n",
    "        \n",
    "        Returns:\n",
    "            N-gram概率\n",
    "        \"\"\"\n",
    "        if self.n == 1:\n",
    "            num_tokens = len(self.tokens)\n",
    "            # n_grams = nltk.ngrams(self.tokens, self.n)\n",
    "            return { (unigram,): (count + self.laplace) /(num_tokens + self.laplace*self.vocabulary_size) for unigram, count in self.vocab.items() }\n",
    "        else:\n",
    "            n_grams = nltk.ngrams(self.tokens, self.n)\n",
    "            n_vocab = nltk.FreqDist(n_grams)\n",
    "\n",
    "            m_grams = nltk.ngrams(self.tokens, self.n-1)\n",
    "            m_vocab = nltk.FreqDist(m_grams)\n",
    "            def smoothed_count(n_gram, n_count):\n",
    "                m_gram = n_gram[:-1]\n",
    "                m_count = m_vocab[m_gram]\n",
    "                return (n_count + self.laplace) / (m_count + self.laplace*self.vocabulary_size)\n",
    "            return { n_gram: smoothed_count(n_gram, count) for n_gram, count in n_vocab.items() }\n",
    "\n",
    "    def change_testtoken(self, ngram):\n",
    "        \"\"\"_summary_\n",
    "            没有出现的字词换为<NUK>\n",
    "        \"\"\"\n",
    "        mask = lambda ngram, bitmask: tuple((token if flag == 1 else \"<UNK>\" for token,flag in zip(ngram, bitmask)))\n",
    "\n",
    "        ngram = (ngram,) if type(ngram) is str else ngram\n",
    "        for possible_known in [mask(ngram, bitmask) for bitmask in self.mask]:\n",
    "            if possible_known in self.model:\n",
    "                return possible_known\n",
    "            \n",
    "    def perplexity(self, test_tokens):\n",
    "        test_ngrams = nltk.ngrams(test_tokens, self.n)\n",
    "        N = len(test_tokens)\n",
    "        known_ngrams  = (self.change_testtoken(ngram) for ngram in test_ngrams)\n",
    "        probabilities = [self.model[ngram] for ngram in known_ngrams]\n",
    "\n",
    "        return math.exp((-1/N) * sum(map(math.log, probabilities)))\n",
    "    \n",
    "    def generate_sentences(self, num, min_len = 12, max_len = 24):\n",
    "        def max_prob_word(prev, i, without=[]):\n",
    "            blacklist = [\"<UNK>\"] + without\n",
    "            candidates = ((ngram[-1],prob) for ngram,prob in self.model.items() if ngram[:-1]==prev)\n",
    "            candidates = filter(lambda candidate: candidate[0] not in blacklist, candidates)\n",
    "            word_list = sorted(candidates, key=lambda candidate: candidate[1], reverse=True)\n",
    "            if len(word_list) == 0:\n",
    "                return (\"</s>\", 1)\n",
    "            else:\n",
    "                return word_list[0 if prev != () and prev[-1] != \"<s>\" else i]\n",
    "        \n",
    "        for i in range(num):\n",
    "            sent, prob = [\"<s>\"] * max(1, self.n-1), 1\n",
    "            while sent[-1] != \"</s>\":\n",
    "                prev = () if self.n == 1 else tuple(sent[-(self.n-1):])\n",
    "                blacklist = sent + ([\"</s>\"] if len(sent) < min_len else [])\n",
    "                next_token, next_prob = max_prob_word(prev, i, without=blacklist)\n",
    "                sent.append(next_token)\n",
    "                prob *= next_prob\n",
    "                \n",
    "                if len(sent) >= max_len:\n",
    "                    sent.append(\"</s>\")\n",
    "\n",
    "            yield ' '.join(sent), -math.log10(prob)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "08d90e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "when laplace = 2 , Loading 1-gram model... Model perplexity: 407.701\n",
      "--------------------------------------------------\n",
      "when laplace = 2 , Loading 2-gram model... Model perplexity: 601.284\n",
      "--------------------------------------------------\n",
      "when laplace = 2 , Loading 3-gram model... Model perplexity: 2069.279\n",
      "--------------------------------------------------\n",
      "when laplace = 4 , Loading 1-gram model... Model perplexity: 411.936\n",
      "--------------------------------------------------\n",
      "when laplace = 4 , Loading 2-gram model... Model perplexity: 879.841\n",
      "--------------------------------------------------\n",
      "when laplace = 4 , Loading 3-gram model... Model perplexity: 3207.460\n",
      "--------------------------------------------------\n",
      "when laplace = 0.1 , Loading 1-gram model... Model perplexity: 403.737\n",
      "--------------------------------------------------\n",
      "when laplace = 0.1 , Loading 2-gram model... Model perplexity: 158.807\n",
      "--------------------------------------------------\n",
      "when laplace = 0.1 , Loading 3-gram model... Model perplexity: 276.914\n",
      "--------------------------------------------------\n",
      "when laplace = 0.01 , Loading 1-gram model... Model perplexity: 403.551\n",
      "--------------------------------------------------\n",
      "when laplace = 0.01 , Loading 2-gram model... Model perplexity: 84.023\n",
      "--------------------------------------------------\n",
      "when laplace = 0.01 , Loading 3-gram model... Model perplexity: 74.791\n"
     ]
    }
   ],
   "source": [
    "for k in [2, 4, 0.1, 0.01]:\n",
    "    for n in [1,2,3]:\n",
    "        print(\"-\" * 50)\n",
    "        train_tokens, train_sentences = token_text(json_objects_train, n, 6)\n",
    "        test_tokens, test_sentences = token_text(json_objects_test, n, 6)\n",
    "        # print(\"Loading {}-gram model...\".format(n))\n",
    "        lm = LanguageModel_2(train_tokens, n, laplace=k)\n",
    "        perplexity = lm.perplexity(test_tokens)\n",
    "        print(\"when laplace = {} , Loading {}-gram model...\".format(k, n), \"Model perplexity: {:.3f}\".format(perplexity))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb8e5a2",
   "metadata": {},
   "source": [
    "可以发现laplace = 0.01效果最好\n",
    "\n",
    "当 Laplace 参数较大（如4）时，模型的困惑度（perplexity）通常会增加，这意味着模型在对文本进行建模时变得更加困难，可能由于平滑过度导致模型过于简化。\n",
    "当 Laplace 参数较小时（如0.01），模型的困惑度通常会减小，这意味着模型更好地适应了训练数据，但也可能导致过拟合。\n",
    "\n",
    "可以生成句子康康效果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6c692252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Loading 1-gram model...\n",
      "<s> the of and to in a was that as for with </s> \n",
      " 句子的-log10(概率)是(21.20661)\n",
      "<s> of and to in a was that as for with is </s> \n",
      " 句子的-log10(概率)是(22.06137)\n",
      "<s> and to in a was that as for with is his of by on The he from In an be at had which </s> \n",
      " 句子的-log10(概率)是(49.06820)\n",
      "<s> to in a was that as for with is his by and on The he from In an be at had which it </s> \n",
      " 句子的-log10(概率)是(50.06814)\n",
      "<s> in a was that as for with is his by on to The he from In an be at had which it are </s> \n",
      " 句子的-log10(概率)是(51.01921)\n",
      "--------------------------------------------------\n",
      "Loading 2-gram model...\n",
      "<s> The film was a new version of the first time in his own and that he had been found to be used for </s> \n",
      " 句子的-log10(概率)是(30.65861)\n",
      "<s> In the first time of a new version was not be used to his own and that he had been found in which </s> \n",
      " 句子的-log10(概率)是(32.64052)\n",
      "<s> He was a new version of the first time in his own and that he had been found to be used for its </s> \n",
      " 句子的-log10(概率)是(31.34877)\n",
      "<s> It is a new version of the first time in his own and was not be used to have been found that he </s> \n",
      " 句子的-log10(概率)是(30.75047)\n",
      "<s> This is a new version of the first time in his own and was not be used to have been found that he </s> \n",
      " 句子的-log10(概率)是(31.22192)\n",
      "--------------------------------------------------\n",
      "Loading 3-gram model...\n",
      "<s> <s> The film was released on October 31, 2014. </s> \n",
      " 句子的-log10(概率)是(10.07352)\n",
      "<s> <s> In the United States and Canada, it is not a single large room which was released on October 31, 2014. </s> \n",
      " 句子的-log10(概率)是(33.37945)\n",
      "<s> <s> He was also a member of the most common cause with </s> \n",
      " 句子的-log10(概率)是(13.88480)\n",
      "<s> <s> It is a major role in the United States and Canada, it had been an important part of his own work on </s> \n",
      " 句子的-log10(概率)是(31.92814)\n",
      "<s> <s> This is the most common cause of death in a letter to his own work on \"The Lord </s> \n",
      " 句子的-log10(概率)是(26.51724)\n"
     ]
    }
   ],
   "source": [
    "for n in [1,2,3]:\n",
    "    print(\"-\" * 50)\n",
    "    train_tokens, train_sentences = token_text(json_objects_train, n, 6)\n",
    "    test_tokens, test_sentences = token_text(json_objects_test, n, 6)\n",
    "    print(\"Loading {}-gram model...\".format(n))\n",
    "    lm = LanguageModel_2(train_tokens, n, laplace=0.01)\n",
    "    for sentence, prob in lm.generate_sentences(5):\n",
    "        print(\"{} \\n 句子的-log10(概率)是({:.5f})\".format(sentence, prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1b4282-d67c-4543-88cf-1bd0518173c2",
   "metadata": {},
   "source": [
    "## Task3 - Build NB/LR classifiers\n",
    "\n",
    "> 1) Build a Naive Bayes classifier (with Laplace smoothing) and test your model on test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cee7ad0",
   "metadata": {},
   "source": [
    "数据处理直接引用LR的Feature 以及词袋，模型代码按照讲义编写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53d7f9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes to here\n",
    "import string\n",
    "import math\n",
    "import numpy as np\n",
    "import json\n",
    "### 数据处理\n",
    "## 直接load LR 的feature使用\n",
    "with open('X_train_dataset.json', 'r') as f:\n",
    "    matrix_list1 = json.load(f)\n",
    "X_train_dataset = np.array(matrix_list1)\n",
    "\n",
    "with open('X_val_dataset.json', 'r') as f:\n",
    "    matrix_list2 = json.load(f)\n",
    "X_val_dataset = np.array(matrix_list2)\n",
    "\n",
    "with open('X_test_dataset.json', 'r') as f:\n",
    "    matrix_list3 = json.load(f)\n",
    "X_test_dataset = np.array(matrix_list3)\n",
    "\n",
    "## 加载标签\n",
    "from sklearn import model_selection\n",
    "X = [] # an element of X is represented as (title,text)\n",
    "Y = [] # an element of Y represents the label of the corresponding X element\n",
    "for i in range(1000):\n",
    "    obj = json.loads(json_objects_train[i])\n",
    "    title = obj.get(\"title\")\n",
    "    text = obj.get(\"text\")\n",
    "    label = obj.get(\"label\")\n",
    "    X.append((title, text))\n",
    "    Y.append(label)\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = model_selection.train_test_split(X, Y, test_size=0.2, random_state=0)\n",
    "\n",
    "Y_test = []\n",
    "for i in range(100):\n",
    "    obj = json.loads(json_objects_test[i])\n",
    "    title = obj.get(\"title\")\n",
    "    text = obj.get(\"text\")\n",
    "    label = obj.get(\"label\")\n",
    "    Y_test.append(label)\n",
    "    \n",
    "## 合并数据\n",
    "X_train_all = np.vstack((X_train_dataset, X_val_dataset))\n",
    "\n",
    "Y_train_all = Y_train + Y_val\n",
    "\n",
    "## 算出所有单词个数\n",
    "V = len(X_train_all[0])\n",
    "\n",
    "class_label = ['Film','Book','Politician','Writer','Food','Actor','Animal','Software','Artist','Disease']\n",
    "class_label_dict = []\n",
    "\n",
    "for i in range(10):\n",
    "    all_num_vc = 0\n",
    "    list_class = [0]*V\n",
    "    for j in range(1000):\n",
    "        if Y_train_all[j] == class_label[i]:\n",
    "            all_num_vc +=  np.sum(X_train_all[j])\n",
    "            list_class += X_train_all[j]\n",
    "    class_label_dict.append([class_label[i], list_class, all_num_vc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "166a3ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NB_train_model():\n",
    "    def __init__(self, dataset, laplace = 1):\n",
    "        self.dataset = dataset\n",
    "        self.laplace = laplace\n",
    "    \n",
    "    def every_class_prior(self, label, class_label, class_label_dict, V):\n",
    "        \"\"\"_summary_\n",
    "            得到一个这个类别词汇的先验概率字典\n",
    "        Args:\n",
    "            label : 需要的类的名字\n",
    "            class_label : 类别标签列表\n",
    "            class_label_dict : 类别标签字典\n",
    "            V : 词汇表大小\n",
    "\n",
    "        Returns:\n",
    "            word_prior_list : 每个单词先验概率列表\n",
    "        \"\"\"\n",
    "        for i in range(10):\n",
    "            if class_label[i] == label:\n",
    "                N_c = class_label_dict[i][2]\n",
    "                list_class_now = class_label_dict[i][1]\n",
    "                word_prior_list = [0]*V\n",
    "        \n",
    "                for j in range(V):\n",
    "                    word_prior_list[j] = (list_class_now[j] + self.laplace) / (N_c + V*self.laplace)\n",
    "                    \n",
    "                return word_prior_list\n",
    "            \n",
    "        # 如果没有找到匹配的类别标签，返回默认值\n",
    "        return None\n",
    "\n",
    "## train p(c)   \n",
    "total = sum(train_classes_count.values())\n",
    "train_class_prob = {key: value / total for key, value in train_classes_count.items()}\n",
    "NB_model = NB_train_model(json_objects_train, laplace=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6b68bc68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct 0 Film\n",
      "correct 1 Film\n",
      "correct 2 Film\n",
      "correct 3 Film\n",
      "correct 4 Film\n",
      "Wrong 5 原来： Film 被分类为： Politician\n",
      "correct 6 Film\n",
      "correct 7 Film\n",
      "correct 8 Film\n",
      "correct 9 Film\n",
      "correct 10 Book\n",
      "Wrong 11 原来： Book 被分类为： Writer\n",
      "Wrong 12 原来： Book 被分类为： Politician\n",
      "correct 13 Book\n",
      "correct 14 Book\n",
      "correct 15 Book\n",
      "correct 16 Book\n",
      "correct 17 Book\n",
      "correct 18 Book\n",
      "Wrong 19 原来： Book 被分类为： Writer\n",
      "correct 20 Politician\n",
      "correct 21 Politician\n",
      "correct 22 Politician\n",
      "correct 23 Politician\n",
      "correct 24 Politician\n",
      "correct 25 Politician\n",
      "correct 26 Politician\n",
      "correct 27 Politician\n",
      "correct 28 Politician\n",
      "correct 29 Politician\n",
      "correct 30 Writer\n",
      "correct 31 Writer\n",
      "Wrong 32 原来： Writer 被分类为： Politician\n",
      "correct 33 Writer\n",
      "correct 34 Writer\n",
      "correct 35 Writer\n",
      "correct 36 Writer\n",
      "correct 37 Writer\n",
      "Wrong 38 原来： Writer 被分类为： Politician\n",
      "correct 39 Writer\n",
      "correct 40 Food\n",
      "correct 41 Food\n",
      "correct 42 Food\n",
      "correct 43 Food\n",
      "correct 44 Food\n",
      "correct 45 Food\n",
      "correct 46 Food\n",
      "correct 47 Food\n",
      "correct 48 Food\n",
      "correct 49 Food\n",
      "correct 50 Actor\n",
      "correct 51 Actor\n",
      "correct 52 Actor\n",
      "correct 53 Actor\n",
      "correct 54 Actor\n",
      "correct 55 Actor\n",
      "Wrong 56 原来： Actor 被分类为： Writer\n",
      "correct 57 Actor\n",
      "correct 58 Actor\n",
      "correct 59 Actor\n",
      "correct 60 Animal\n",
      "correct 61 Animal\n",
      "correct 62 Animal\n",
      "correct 63 Animal\n",
      "correct 64 Animal\n",
      "correct 65 Animal\n",
      "correct 66 Animal\n",
      "correct 67 Animal\n",
      "correct 68 Animal\n",
      "correct 69 Animal\n",
      "correct 70 Software\n",
      "correct 71 Software\n",
      "correct 72 Software\n",
      "correct 73 Software\n",
      "correct 74 Software\n",
      "correct 75 Software\n",
      "correct 76 Software\n",
      "correct 77 Software\n",
      "correct 78 Software\n",
      "correct 79 Software\n",
      "correct 80 Artist\n",
      "correct 81 Artist\n",
      "correct 82 Artist\n",
      "correct 83 Artist\n",
      "correct 84 Artist\n",
      "correct 85 Artist\n",
      "correct 86 Artist\n",
      "correct 87 Artist\n",
      "correct 88 Artist\n",
      "correct 89 Artist\n",
      "correct 90 Disease\n",
      "correct 91 Disease\n",
      "correct 92 Disease\n",
      "correct 93 Disease\n",
      "correct 94 Disease\n",
      "correct 95 Disease\n",
      "correct 96 Disease\n",
      "correct 97 Disease\n",
      "correct 98 Disease\n",
      "correct 99 Disease\n",
      "0.93\n"
     ]
    }
   ],
   "source": [
    "## 测试\n",
    "import math\n",
    "def test_on_model(model, test_dataset):\n",
    "    correct_num = 0\n",
    "    label_predict = []\n",
    "    for i in range(100):\n",
    "        model_label = 'None'\n",
    "        model_prob = float(\"-inf\")\n",
    "        test_list = test_dataset[i]\n",
    "        \n",
    "        for class_name in train_classes_count.keys():\n",
    "            model_prob_now = math.log2(train_class_prob[class_name])\n",
    "            word_prior_list = model.every_class_prior(class_name, class_label, class_label_dict, V) ## 词汇概率字典\n",
    "            \n",
    "            for j in range(V):\n",
    "                if word_prior_list[j] != 0:\n",
    "                    model_prob_now += (math.log2(word_prior_list[j]))*test_list[j]\n",
    "                \n",
    "            ## 判断是否是最大的\n",
    "            if model_prob_now > model_prob:\n",
    "                model_prob = model_prob_now\n",
    "                model_label = class_name\n",
    "        \n",
    "        ## 判断准确率\n",
    "        if model_label == Y_test[i]:\n",
    "            correct_num += 1\n",
    "            print(\"correct\", i, Y_test[i])\n",
    "            label_predict.append(model_label)\n",
    "        else:\n",
    "            print(\"Wrong\", i, \"原来：\",Y_test[i], \"被分类为：\",model_label)\n",
    "            label_predict.append(model_label)\n",
    "        \n",
    "    return correct_num/100, label_predict\n",
    "\n",
    "accuracy, label_predict = test_on_model(NB_model, X_test_dataset)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2e3c29",
   "metadata": {},
   "source": [
    "可以看到准确率为0.93\n",
    "\n",
    "另：如果改变laplace值，可以看到效果差异不大"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ec962af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- laplace为 2 ---------------\n",
      "被分错的\n",
      "Wrong 5 原来： Film 被分类为： Politician\n",
      "Wrong 11 原来： Book 被分类为： Writer\n",
      "Wrong 12 原来： Book 被分类为： Politician\n",
      "Wrong 19 原来： Book 被分类为： Writer\n",
      "Wrong 32 原来： Writer 被分类为： Politician\n",
      "Wrong 38 原来： Writer 被分类为： Politician\n",
      "Wrong 56 原来： Actor 被分类为： Writer\n",
      "Wrong 59 原来： Actor 被分类为： Film\n",
      "0.92 laplace = 2\n",
      "--------------- laplace为 4 ---------------\n",
      "被分错的\n",
      "Wrong 5 原来： Film 被分类为： Politician\n",
      "Wrong 11 原来： Book 被分类为： Writer\n",
      "Wrong 12 原来： Book 被分类为： Politician\n",
      "Wrong 16 原来： Book 被分类为： Writer\n",
      "Wrong 19 原来： Book 被分类为： Writer\n",
      "Wrong 32 原来： Writer 被分类为： Politician\n",
      "Wrong 38 原来： Writer 被分类为： Politician\n",
      "Wrong 50 原来： Actor 被分类为： Film\n",
      "Wrong 51 原来： Actor 被分类为： Film\n",
      "Wrong 52 原来： Actor 被分类为： Film\n",
      "Wrong 53 原来： Actor 被分类为： Film\n",
      "Wrong 54 原来： Actor 被分类为： Film\n",
      "Wrong 55 原来： Actor 被分类为： Film\n",
      "Wrong 56 原来： Actor 被分类为： Writer\n",
      "Wrong 57 原来： Actor 被分类为： Film\n",
      "Wrong 58 原来： Actor 被分类为： Film\n",
      "Wrong 59 原来： Actor 被分类为： Film\n",
      "Wrong 60 原来： Animal 被分类为： Disease\n",
      "0.82 laplace = 4\n",
      "--------------- laplace为 0.5 ---------------\n",
      "被分错的\n",
      "Wrong 5 原来： Film 被分类为： Politician\n",
      "Wrong 11 原来： Book 被分类为： Writer\n",
      "Wrong 12 原来： Book 被分类为： Politician\n",
      "Wrong 19 原来： Book 被分类为： Writer\n",
      "Wrong 32 原来： Writer 被分类为： Politician\n",
      "Wrong 38 原来： Writer 被分类为： Politician\n",
      "Wrong 56 原来： Actor 被分类为： Writer\n",
      "0.93 laplace = 0.5\n",
      "--------------- laplace为 0.25 ---------------\n",
      "被分错的\n",
      "Wrong 5 原来： Film 被分类为： Politician\n",
      "Wrong 11 原来： Book 被分类为： Writer\n",
      "Wrong 12 原来： Book 被分类为： Politician\n",
      "Wrong 19 原来： Book 被分类为： Writer\n",
      "Wrong 32 原来： Writer 被分类为： Politician\n",
      "Wrong 38 原来： Writer 被分类为： Politician\n",
      "Wrong 56 原来： Actor 被分类为： Writer\n",
      "0.93 laplace = 0.25\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import math\n",
    "def test_on_model(model, test_dataset):\n",
    "    correct_num = 0\n",
    "    # label_predict = []\n",
    "    for i in range(100):\n",
    "        model_label = 'None'\n",
    "        model_prob = float(\"-inf\")\n",
    "        test_list = test_dataset[i]\n",
    "        \n",
    "        for class_name in train_classes_count.keys():\n",
    "            model_prob_now = math.log2(train_class_prob[class_name])\n",
    "            word_prior_list = model.every_class_prior(class_name, class_label, class_label_dict, V) ## 词汇概率字典\n",
    "            \n",
    "            for j in range(V):\n",
    "                if word_prior_list[j] != 0:\n",
    "                    model_prob_now += (math.log2(word_prior_list[j]))*test_list[j]\n",
    "                \n",
    "            ## 判断是否是最大的\n",
    "            if model_prob_now > model_prob:\n",
    "                model_prob = model_prob_now\n",
    "                model_label = class_name\n",
    "        \n",
    "        # model_label = max(predict_prob.items(), key=lambda x:x[1])[0]\n",
    "        ## 判断准确率\n",
    "        if model_label == Y_test[i]:\n",
    "            correct_num += 1\n",
    "        #     print(\"correct\", i, Y_test[i])\n",
    "        #     label_predict.append(model_label)\n",
    "        else:\n",
    "            print(\"Wrong\", i, \"原来：\",Y_test[i], \"被分类为：\",model_label)\n",
    "            label_predict.append(model_label)\n",
    "        \n",
    "    return correct_num/100\n",
    "\n",
    "for k in [2, 4, 0.5, 0.25]:\n",
    "    NB_model = NB_train_model(json_objects_train, laplace=k)\n",
    "    print(\"-\"*15, \"laplace为\", k, \"-\"*15)\n",
    "    print(\"被分错的\")\n",
    "    accuracy = test_on_model(NB_model, X_test_dataset)\n",
    "    print(accuracy, \"laplace =\", k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988071af",
   "metadata": {},
   "source": [
    "可以看到被分错的还是那几篇文章"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca80a4a-9db5-426f-a45a-b1c2c1e34423",
   "metadata": {},
   "source": [
    "> 2) Build a LR classifier. This question seems to be challenging. We did not directly provide features for samples. But just use your own method to build useful features. You may need to split the training dataset into train and validation so that some involved parameters can be tuned. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6dd030",
   "metadata": {},
   "source": [
    "数据处理代码参考老师的L03文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "1cde80d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes to here\n",
    "from sklearn import model_selection\n",
    "import numpy as np\n",
    "import nltk\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "### 数据处理代码参考老师的L03文件\n",
    "stopwords = ['a', 'about', 'above', 'across', 'after', 'afterwards', 'again', 'against', 'all', 'almost', 'alone',\n",
    "             'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'amoungst', 'amount',\n",
    "             'an', 'and', 'another', 'any', 'anyhow', 'anyone', 'anything', 'anyway', 'anywhere', 'are', 'around',\n",
    "             'as', 'at', 'back', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before',\n",
    "             'beforehand', 'behind', 'being', 'below', 'beside', 'besides', 'between', 'beyond', 'bill', 'both',\n",
    "             'bottom', 'but', 'by', 'call', 'can', 'cannot', 'cant', 'co', 'con', 'could', 'couldnt', 'cry', 'de',\n",
    "             'describe', 'detail', 'did', 'do', 'does', 'doing', 'don', 'done', 'down', 'due', 'during', 'each', 'eg',\n",
    "             'eight', 'either', 'eleven', 'else', 'elsewhere', 'empty', 'enough', 'etc', 'even', 'ever', 'every', 'everyone',\n",
    "             'everything', 'everywhere', 'except', 'few', 'fifteen', 'fify', 'fill', 'find', 'fire', 'first', 'five', 'for',\n",
    "             'former', 'formerly', 'forty', 'found', 'four', 'from', 'front', 'full', 'further', 'get', 'give', 'go', 'had',\n",
    "             'has', 'hasnt', 'have', 'having', 'he', 'hence', 'her', 'here', 'hereafter', 'hereby', 'herein', 'hereupon',\n",
    "             'hers', 'herself', 'him', 'himself', 'his', 'how', 'however', 'hundred', 'i', 'ie', 'if', 'in', 'inc', 'indeed',\n",
    "             'interest', 'into', 'is', 'it', 'its', 'itself', 'just', 'keep', 'last', 'latter', 'latterly', 'least', 'less',\n",
    "             'ltd', 'made', 'many', 'may', 'me', 'meanwhile', 'might', 'mill', 'mine', 'more', 'moreover', 'most', 'mostly',\n",
    "             'move', 'much', 'must', 'my', 'myself', 'name', 'namely', 'neither', 'never', 'nevertheless', 'next', 'nine',\n",
    "             'no', 'nobody', 'none', 'noone', 'nor', 'not', 'nothing', 'now', 'nowhere', 'of', 'off', 'often', 'on', 'once',\n",
    "             'one', 'only', 'onto', 'or', 'other', 'others', 'otherwise', 'our', 'ours', 'ourselves', 'out', 'over', 'own',\n",
    "             'part', 'per', 'perhaps', 'please', 'put', 'rather', 're', 's', 'same', 'see', 'seem', 'seemed', 'seeming',\n",
    "             'seems', 'serious', 'several', 'she', 'should', 'show', 'side', 'since', 'sincere', 'six', 'sixty', 'so', \n",
    "             'some', 'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhere', 'still', 'such', 'system',\n",
    "             't', 'take', 'ten', 'than', 'that', 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'thence', 'there',\n",
    "             'thereafter', 'thereby', 'therefore', 'therein', 'thereupon', 'these', 'they', 'thickv', 'thin', 'third', 'this',\n",
    "             'those', 'though', 'three', 'through', 'throughout', 'thru', 'thus', 'to', 'together', 'too', 'top', 'toward',\n",
    "             'towards', 'twelve', 'twenty', 'two', 'un', 'under', 'until', 'up', 'upon', 'us', 'very', 'via', 'was', 'we',\n",
    "             'well', 'were', 'what', 'whatever', 'when', 'whence', 'whenever', 'where', 'whereafter', 'whereas', 'whereby',\n",
    "             'wherein', 'whereupon', 'wherever', 'whether', 'which', 'while', 'whither', 'who', 'whoever', 'whole', 'whom',\n",
    "             'whose', 'why', 'will', 'with', 'within', 'without', 'would', 'yet', 'you', 'your', 'yours', 'yourself',\n",
    "             'yourselves']\n",
    "\n",
    "## 分出train 和 val\n",
    "X = [] # an element of X is represented as (title,text)\n",
    "Y = [] # an element of Y represents the label of the corresponding X element\n",
    "for i in range(1000):\n",
    "    obj = json.loads(json_objects_train[i])\n",
    "    title = obj.get(\"title\")\n",
    "    text = obj.get(\"text\")\n",
    "    label = obj.get(\"label\")\n",
    "    X.append((title, text))\n",
    "    Y.append(label)\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = model_selection.train_test_split(X, Y, test_size=0.2, random_state=0)\n",
    "\n",
    "## 选出所有feature\n",
    "feature_word = []\n",
    "for i in range(len(X_train)):\n",
    "    text = X_train[i][1]\n",
    "    tt = nltk.word_tokenize(text)\n",
    "    tokens = [word for word in (word.strip(string.punctuation).lower() for word in tt) if word != '']\n",
    "    # tokenized_word.extend(tokens)\n",
    "    feature_word.extend(tokens)\n",
    "vocab = nltk.FreqDist(feature_word)\n",
    "feature_word = [token for token in feature_word if vocab[token] > 5 ] ## 限制大于5的频率才算\n",
    "feature_word = [word for word in feature_word if word not in stopwords]\n",
    "# print(len(set(feature_word))) \n",
    "# 37507\n",
    "feature = list(set(feature_word))\n",
    "\n",
    "# 把train_dataset 和 Val_dataset 变成向量（后边变成tensor）\n",
    "X_train_dataset = np.zeros((len(X_train),len(feature)))\n",
    "for i in range(len(X_train)):\n",
    "    text = X_train[i][1]\n",
    "    tt = nltk.word_tokenize(text)\n",
    "    tokens = [word for word in (word.strip(string.punctuation).lower() for word in tt) if word != '']\n",
    "    vocab = nltk.FreqDist(tokens)\n",
    "    for word in vocab:\n",
    "        if word in feature:\n",
    "            X_train_dataset[i][feature.index(word)] = vocab[word]\n",
    "            \n",
    "# ## 保存一下 跑一次要好久\n",
    "# matrix_list = X_train_dataset.tolist()\n",
    "# with open('X_train_dataset.json', 'w') as f:\n",
    "#     json.dump(matrix_list, f)\n",
    "\n",
    "# ## 重新加载\n",
    "\n",
    "# with open('X_train_dataset.json', 'r') as f:\n",
    "#     matrix_list = json.load(f)\n",
    "\n",
    "# # 将数据转换为矩阵\n",
    "# matrix = np.array(matrix_list)\n",
    "\n",
    "X_val_dataset = np.zeros((len(X_val),len(feature)))\n",
    "for i in range(len(X_val)):\n",
    "    text = X_val[i][1]\n",
    "    tt = nltk.word_tokenize(text)\n",
    "    tokens = [word for word in (word.strip(string.punctuation).lower() for word in tt) if word != '']\n",
    "    vocab = nltk.FreqDist(tokens)\n",
    "    for word in vocab:\n",
    "        if word in feature:\n",
    "            X_val_dataset[i][feature.index(word)] = vocab[word]\n",
    "            \n",
    "# matrix_list2 = X_val_dataset.tolist()\n",
    "# with open('X_val_dataset.json', 'w') as f:\n",
    "#     json.dump(matrix_list2, f)\n",
    "\n",
    "## 转换为tensor形式\n",
    "X_train_dataset_tensor = torch.from_numpy(X_train_dataset)\n",
    "X_val_dataset_tensor = torch.from_numpy(X_val_dataset)\n",
    "tensor_list_train = torch.cat([X_train_dataset_tensor[i, :].unsqueeze(0) for i in range(800)], dim=0)\n",
    "tensor_list_val = torch.cat([X_val_dataset_tensor[i, :].unsqueeze(0) for i in range(200)],dim = 0)\n",
    "\n",
    "## 把标签换成整数索引\n",
    "class_label = ['Film','Book','Politician','Writer','Food','Actor','Animal','Software','Artist','Disease']\n",
    "int_label = [5,3,7,9,6,0,1,8,2,4]\n",
    "label_encoder = LabelEncoder()\n",
    "Y_train_indices = label_encoder.fit_transform(Y_train)\n",
    "Y_val_indices = label_encoder.fit_transform(Y_val)\n",
    "# ## 转换回来\n",
    "# original_labels = label_encoder.inverse_transform(Y_val_indices)\n",
    "\n",
    "## 传入数据\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "train_dataset = Dataset(tensor_list_train, Y_train_indices)\n",
    "val_dataset = Dataset(tensor_list_val, Y_val_indices)\n",
    "\n",
    "## 超参数\n",
    "batch_size = 100\n",
    "lr_rate = 0.01\n",
    "num_epochs = 2\n",
    "input_size = 37507\n",
    "output_size = 10\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=50, shuffle=False)\n",
    "\n",
    "\n",
    "## 构建模型\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "model = LogisticRegression(input_size, output_size)\n",
    "model = model.to(dtype=torch.float64)\n",
    "# 损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr_rate)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for data in train_loader:\n",
    "        imgs, targets = data\n",
    "        output = model(imgs)\n",
    "        loss = criterion(output, targets)\n",
    "\n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 打印epoch损失\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')\n",
    "\n",
    "# 保留参数\n",
    "# torch.save(model.state_dict(), 'model.pth')\n",
    "\n",
    "# 在验证集上评估\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print(\"Validation Accuracy:\", accuracy)\n",
    "# 算出来0.96"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53a8b62-1ef4-4c2f-934d-f78551ad039e",
   "metadata": {},
   "source": [
    "> 3) Report Micro-F1 score and Macro-F1 score for these classifiers on testing dataset explain our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9c731e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- NB model ---------------\n",
      "Class Film: Precision=1.0, Recall=0.9, F1=0.9473684210526316\n",
      "Class Book: Precision=1.0, Recall=1.0, F1=1.0\n",
      "Class Politician: Precision=1.0, Recall=1.0, F1=1.0\n",
      "Class Writer: Precision=1.0, Recall=0.7, F1=0.8235294117647058\n",
      "Class Food: Precision=1.0, Recall=1.0, F1=1.0\n",
      "Class Actor: Precision=1.0, Recall=0.9, F1=0.9473684210526316\n",
      "Class Animal: Precision=1.0, Recall=1.0, F1=1.0\n",
      "Class Software: Precision=0.7142857142857143, Recall=1.0, F1=0.8333333333333333\n",
      "Class Artist: Precision=1.0, Recall=1.0, F1=1.0\n",
      "Class Disease: Precision=0.7272727272727273, Recall=0.8, F1=0.761904761904762\n",
      "Micro-F1 Score of LR model: 0.93\n",
      "Macro-F1 Score of LR model: 0.9313504349108065\n"
     ]
    }
   ],
   "source": [
    "# Your code goes to here\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "## 对于NB模型\n",
    "print(\"-\"*15, \"NB model\", \"-\"*15)\n",
    "Y1 = []\n",
    "for i in range(100):\n",
    "    obj = json.loads(json_objects_test[i])\n",
    "    title = obj.get(\"title\")\n",
    "    text = obj.get(\"text\")\n",
    "    label = obj.get(\"label\")\n",
    "    Y1.append(label)\n",
    "    \n",
    "# 计算每个类精确度、召回率和 F1 分数\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(Y1, label_predict, average=None)\n",
    "\n",
    "# 计算 Micro-F1 分数\n",
    "micro_precision, micro_recall, micro_f1, _ = precision_recall_fscore_support(Y1, label_predict, average='micro')\n",
    "\n",
    "macro_f1 = np.mean(f1)\n",
    "\n",
    "for i in range(output_size):\n",
    "    print(f'Class {class_label[i]}: Precision={precision[i]}, Recall={recall[i]}, F1={f1[i]}')\n",
    "\n",
    "print(f'Micro-F1 Score of LR model: {micro_f1}')\n",
    "print(f'Macro-F1 Score of LR model: {macro_f1}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee7f1a5",
   "metadata": {},
   "source": [
    "### 对于NB模型：\n",
    "1. 对于部分类别（Class Book, Politician, Food, Animal, Artist），模型的精确度、召回率和 F1 分数都达到了 1.0，这意味着模型在这些类别上的预测非常准确，并且没有漏掉任何样本。\n",
    "   \n",
    "2. 在其他几个类别（Class Film, Writer, Actor, Software, Disease）上，模型的性能稍有下降。比如，在 Class Film, Writer, Actor 上，模型的召回率较低，表明模型未能捕获到所有正类样本。在 Class Software 上，模型的精确度稍低，表明模型在该类别上可能存在一些误报，而在 Class Disease 上，模型的精确度稍低，召回率也稍低，表明模型在该类别上的性能较差。\n",
    "   \n",
    "3. 总体而言，Micro-F1 分数为 0.93，Macro-F1 分数为 0.931。Micro-F1 分数考虑了所有类别，并给予每个类别相同的权重。在这个结果中，Micro-F1 分数为0.93，表明模型在整体上具有较好的性能。Macro-F1 分数是所有类别 F1 分数的平均值，它平等对待每个类别。在这个结果中，Macro-F1 分数为0.931，说明模型在每个类别上的较为性能均衡，没有某个类别的性能明显偏低。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "500abc8d-0e08-4a84-af6c-9d0e1e0fa035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- LR model ---------------\n",
      "Class 0: Precision=1.0, Recall=1.0, F1=1.0\n",
      "Class 1: Precision=1.0, Recall=1.0, F1=1.0\n",
      "Class 2: Precision=1.0, Recall=1.0, F1=1.0\n",
      "Class 3: Precision=0.8, Recall=0.8, F1=0.8000000000000002\n",
      "Class 4: Precision=1.0, Recall=1.0, F1=1.0\n",
      "Class 5: Precision=1.0, Recall=0.9, F1=0.9473684210526316\n",
      "Class 6: Precision=1.0, Recall=1.0, F1=1.0\n",
      "Class 7: Precision=0.9090909090909091, Recall=1.0, F1=0.9523809523809523\n",
      "Class 8: Precision=0.9090909090909091, Recall=1.0, F1=0.9523809523809523\n",
      "Class 9: Precision=0.8888888888888888, Recall=0.8, F1=0.8421052631578948\n",
      "Micro-F1 Score of LR model: 0.9500000000000001\n",
      "Macro-F1 Score of LR model: 0.9494235588972432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kim\\AppData\\Local\\Temp\\ipykernel_22076\\2623391384.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(self.linear(x))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "### 对于LR模型\n",
    "print(\"-\"*15, \"LR model\", \"-\"*15)\n",
    "\n",
    "# 打印模型参数\n",
    "# for name, param in model.named_parameters():\n",
    "#     print(f'Parameter name: {name}, Size: {param.size()}, Values: {param}')\n",
    "    \n",
    "## 加载测试数据\n",
    "import numpy as np\n",
    "X1 = []\n",
    "Y1 = []\n",
    "for i in range(100):\n",
    "    obj = json.loads(json_objects_test[i])\n",
    "    title = obj.get(\"title\")\n",
    "    text = obj.get(\"text\")\n",
    "    label = obj.get(\"label\")\n",
    "    X1.append((title, text))\n",
    "    Y1.append(label)\n",
    "\n",
    "# 从 JSON 文件中加载数据\n",
    "with open('X_test_dataset.json', 'r') as f:\n",
    "    matrix_list = json.load(f)\n",
    "\n",
    "# 将数据转换为矩阵\n",
    "X_test_dataset = np.array(matrix_list)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "## 处理数据\n",
    "X_test_dataset_tensor = torch.from_numpy(X_test_dataset)\n",
    "tensor_list_test = torch.cat([X_test_dataset_tensor[i, :].unsqueeze(0) for i in range(100)],dim = 0)\n",
    "Y_test_indices = label_encoder.fit_transform(Y1)\n",
    "test_dataset = Dataset(tensor_list_test, Y_test_indices)\n",
    "test_loader = DataLoader(test_dataset, batch_size=50, shuffle=False)\n",
    "\n",
    "## 准备模型和数据加载\n",
    "# 准备数据\n",
    "input_size = 37507\n",
    "output_size = 10\n",
    "model = LogisticRegression(input_size, output_size)\n",
    "model.load_state_dict(torch.load('model.pth'))\n",
    "\n",
    "## 计算\n",
    "model.eval()\n",
    "model = model.to(dtype=torch.float64)\n",
    "\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        inputs, label = data\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_predictions.extend(predicted.numpy())\n",
    "        all_labels.extend(label.numpy())\n",
    "\n",
    "# 计算每个类精确度、召回率和 F1 分数\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_predictions, average=None)\n",
    "\n",
    "# 计算 Micro-F1 分数\n",
    "micro_precision, micro_recall, micro_f1, _ = precision_recall_fscore_support(all_labels, all_predictions, average='micro')\n",
    "\n",
    "macro_f1 = np.mean(f1)\n",
    "\n",
    "for i in range(output_size):\n",
    "    print(f'Class {i}: Precision={precision[i]}, Recall={recall[i]}, F1={f1[i]}')\n",
    "\n",
    "print(f'Micro-F1 Score of LR model: {micro_f1}')\n",
    "print(f'Macro-F1 Score of LR model: {macro_f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30022064",
   "metadata": {},
   "source": [
    "### 对于LR模型：\n",
    "1. 对于大多数类别（Class 0, 1, 2, 4, 6），模型的精确度、召回率和 F1 分数都达到了 1.0，这意味着模型在这些类别上的预测非常准确，并且没有漏掉任何样本。\n",
    "   \n",
    "2. 在其他几个类别（Class 3, 5, 7, 8, 9）上，模型的性能稍有下降。比如，在 Class 3, 5, 9 上，模型的召回率较低，表明模型未能捕获到所有正类样本。在 Class 1, 3, 7, 8 上，模型的精确度稍低，表明模型在该类别上可能存在一些误报，而在 Class 9 上，模型的精确度稍低，召回率也稍低，表明模型在该类别上的性能较差。\n",
    "   \n",
    "3. 总体而言，Micro-F1 分数为 0.95，Macro-F1 分数为 0.9494。Micro-F1 分数考虑了所有类别，并给予每个类别相同的权重。在这个结果中，Micro-F1 分数为0.95，表明模型在整体上具有较好的性能。Macro-F1 分数是所有类别 F1 分数的平均值，它平等对待每个类别。在这个结果中，Macro-F1 分数为0.9494，说明模型在每个类别上的较为性能均衡，没有某个类别的性能明显偏低。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93c9710",
   "metadata": {},
   "source": [
    "总的来说，我建立的LR模型效果比NB模型要好"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
